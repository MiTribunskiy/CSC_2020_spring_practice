{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# search queries\n",
    "import json\n",
    "import urllib\n",
    "\n",
    "# store data\n",
    "import pickle\n",
    "\n",
    "# Longest Common Subsequence\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# common tokens/words\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# from pprint import PrettyPrinter\n",
    "# pretty = PrettyPrinter(width=30)\n",
    "# pprint = pretty.pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric functions\n",
    "\n",
    "def R_precision_score(results, label):\n",
    "    R = N_true[label]\n",
    "    ctr = 0\n",
    "    for i, res in enumerate(results):\n",
    "        if i == R:\n",
    "            break\n",
    "        opt = res['option']\n",
    "        if options[opt] == label:\n",
    "            ctr += 1\n",
    "    return ctr / R\n",
    "\n",
    "def diff(a, b):\n",
    "    if isinstance(a, tuple):\n",
    "        return tuple(x - y for x, y in zip(a, b))\n",
    "    else:\n",
    "        return a - b\n",
    "\n",
    "def recall_score(results, label, threshold=0.8):\n",
    "    R = N_true[label]\n",
    "    ctr = 0\n",
    "    \n",
    "    if isinstance(threshold, str) and threshold.startswith('cluster'):\n",
    "        size_limit = len(results)\n",
    "        if threshold[7:].isnumeric():\n",
    "            size_limit = min(int(threshold[7:]), size_limit)\n",
    "        max_dist, size = None, 0\n",
    "        for i in reversed(range(1, size_limit)):\n",
    "            curr_dist = diff(results[i - 1]['score'], \n",
    "                             results[i]['score'])\n",
    "            if not max_dist or curr_dist > max_dist:\n",
    "                max_dist = curr_dist\n",
    "                size = i\n",
    "    else:\n",
    "        if isinstance(results[0]['score'], tuple):\n",
    "            threshold = (threshold, -float('inf'))\n",
    "        for i, res in enumerate(results):\n",
    "            if res['score'] < threshold:\n",
    "                size = i\n",
    "                break\n",
    "\n",
    "    for i in range(size):\n",
    "        opt = results[i]['option']\n",
    "        if options[opt] == label:\n",
    "            ctr += 1\n",
    "    return ctr / R, size\n",
    "\n",
    "def roc_score(results, label):\n",
    "    y_score = [res['score'] for res in results]\n",
    "    if isinstance(y_score[0], tuple):\n",
    "        compressor = defaultdict(list)\n",
    "        for i, score in enumerate(y_score):\n",
    "            compressor[score].append(i)\n",
    "        for code, score in enumerate(sorted(compressor)):\n",
    "            for i in compressor[score]:\n",
    "                y_score[i] = code                \n",
    "    y_true = [int(options[res['option']] == label) for res in results]    \n",
    "    return roc_auc_score(y_true, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search queries\n",
    "\n",
    "class Google:\n",
    "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "    api_key = open('.api_key').read()\n",
    "    parameters = {\n",
    "        'limit': 1,\n",
    "        'indent': True,\n",
    "        'key': api_key,\n",
    "    }\n",
    "    \n",
    "    def get_identities(self, queries, extra_params={}):\n",
    "        params = dict(self.parameters)\n",
    "        params.update(extra_params)\n",
    "        \n",
    "        identities_dict = {}\n",
    "        for q in queries:\n",
    "            params['query'] = q\n",
    "            url = self.service_url + '?' + urllib.parse.urlencode(params)\n",
    "            response = json.loads(urllib.request.urlopen(url).read())\n",
    "            for element in response['itemListElement']:\n",
    "                identity = element['result']['name']\n",
    "                score = element['resultScore']\n",
    "                break\n",
    "            else:\n",
    "                identity = ''\n",
    "                score = -1\n",
    "            # score is currently not used\n",
    "            identities_dict[q] = identity\n",
    "        return identities_dict\n",
    "    \n",
    "    def get_prefix_identities(self, queries, extra_params={}):\n",
    "        params = dict(self.parameters)\n",
    "        params.update(extra_params)\n",
    "        \n",
    "        identities_dict = defaultdict(list)\n",
    "        for q in queries:\n",
    "            tokens = list(re.split('[^a-zA-Z0-9]+', q))\n",
    "            for prefix_size in reversed(range(1, len(tokens) + 1)):\n",
    "                params['query'] = ' '.join(tokens[:prefix_size])\n",
    "                url = self.service_url + '?' + urllib.parse.urlencode(params)\n",
    "                response = json.loads(urllib.request.urlopen(url).read())\n",
    "                identities = [''] * params['limit']\n",
    "                for i, element in enumerate(response['itemListElement']):\n",
    "                    identities[i] = element['result']['name']\n",
    "                    score = element['resultScore'] # score is currently not used\n",
    "                identities_dict[q].extend(identities)\n",
    "        return identities_dict\n",
    "    \n",
    "    def get_prefix_identities_extended(self, queries, extra_params={}):\n",
    "        params = dict(self.parameters)\n",
    "        params.update(extra_params)\n",
    "        \n",
    "        identities_dict = defaultdict(list)\n",
    "        for q in queries:\n",
    "            tokens = list(re.split('[^a-zA-Z0-9]+', q))\n",
    "            for prefix_size in reversed(range(1, len(tokens) + 1)):\n",
    "                params['query'] = ' '.join(tokens[:prefix_size])\n",
    "                url = self.service_url + '?' + urllib.parse.urlencode(params)\n",
    "                response = json.loads(urllib.request.urlopen(url).read())\n",
    "                identities = [''] * (params['limit']<<1)\n",
    "                for i, element in enumerate(response['itemListElement']):\n",
    "                    identities[i<<1] = element['result']['name']\n",
    "                    try:\n",
    "                        identities[i<<1|1] = element['result']['detailedDescription']['articleBody']\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "                    score = element['resultScore'] # score is currently not used\n",
    "                identities_dict[q].extend(identities)\n",
    "        return identities_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance metrics\n",
    "\n",
    "class Distance:\n",
    "    valid = {'lcs':'Longest common subsequence',\n",
    "             'fuzzy': 'Common tokens/words',              \n",
    "             'prefix': 'fuzzy for all prefixes (~ wildcard)'}\n",
    "    \n",
    "    def __init__(self, name, acc=4):\n",
    "        if name not in self.valid:\n",
    "            raise ValueError(f'Supported distance metrics: {\", \".join(self.valid)}')\n",
    "        self.name = name\n",
    "        self.acc = acc\n",
    "    \n",
    "    def score(self, query, option):\n",
    "        dist_func = eval(f'self.{self.name}')\n",
    "        return dist_func(query, option)\n",
    "    \n",
    "    def fuzzy(self, query, option):\n",
    "        score = fuzz.token_set_ratio(query, option) / 100\n",
    "        return round(score, self.acc), option\n",
    "    \n",
    "    def lcs(self, query, option):\n",
    "        score = SequenceMatcher(None, query, option).ratio()\n",
    "        return round(score, self.acc), option\n",
    "    \n",
    "    def prefix(self, query, option):\n",
    "        '''\n",
    "        Maximum among prefixes.\n",
    "        The less tokens dropped, the better.\n",
    "        '''\n",
    "        best = [0, 0]\n",
    "        best_option = ''\n",
    "        for n_droped, opt in enumerate(option):\n",
    "            score = fuzz.token_set_ratio(query, opt) / 100\n",
    "            if best[0] < score:\n",
    "                best[0] = score\n",
    "                best[1] = -n_droped\n",
    "                best_option = opt\n",
    "        best[0] = round(best[0], self.acc)\n",
    "        return tuple(best), best_option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def template(*x):\n",
    "    return '{:35}{:15}{:10}{:35}'.format(*map(lambda x: x[:34], map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(queries, distance_name='fuzzy', query_identities={}, option_identities={}, threshold=0.8, \n",
    "                  show_extra=2, method_name=None):\n",
    "    distance = Distance(distance_name)\n",
    "    key_order = ['option', 'score', 'isTrue', 'identity']\n",
    "    \n",
    "    print(template(*key_order))\n",
    "    print('=' * 70)\n",
    "    \n",
    "    for i, q in enumerate(queries):\n",
    "        label = i + 1\n",
    "        q_id = query_identities.get(q, q)\n",
    "        results = []\n",
    "        res = {}\n",
    "        for opt, lab in options.items():\n",
    "            opt_id = option_identities.get(opt, opt)\n",
    "            score, best_opt_id = distance.score(q_id, opt_id)\n",
    "            res['option'] = opt\n",
    "            res['score'] = score\n",
    "            res['isTrue'] = int(lab == label)\n",
    "            res['identity'] = best_opt_id\n",
    "            results.append(res.copy())\n",
    "        results.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "        print(template(f'{q} (query)', '-', '-', q_id))\n",
    "        print('-' * 70)\n",
    "        \n",
    "        R = N_true[label]\n",
    "        for i in range(min(R + show_extra, n)):\n",
    "            res = results[i]\n",
    "            if i == R:\n",
    "                print('-' * 70)\n",
    "            print(template(*(res[key] for key in key_order)))\n",
    "\n",
    "        recall, sample_size = recall_score(results, label=label, threshold=threshold)\n",
    "        prec = R_precision_score(results, label=label)        \n",
    "        roc = roc_score(results, label=label)\n",
    "\n",
    "        print('-' * 70)\n",
    "        print(f'Recall (t={threshold}, size={sample_size}): {recall:.4f}')\n",
    "        print(f'R-precision: {prec:.4f}')\n",
    "        print(f'ROC AUC: {roc:.4f}')\n",
    "        print('=' * 70)\n",
    "        \n",
    "        if method_name:\n",
    "            for query in queries:\n",
    "                Results().update(method_name, query, 'recall', recall)\n",
    "                Results().update(method_name, query, 'Rprec', prec)\n",
    "                Results().update(method_name, query, 'ROC', roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key words preprocessing\n",
    "\n",
    "class JunkContainer:\n",
    "    def __init__(self, junk_list=[]):\n",
    "        self.junk_list = set(junk_list)\n",
    "    \n",
    "    def isin(self, token):\n",
    "        return token in self.junk_list\n",
    "    \n",
    "    def clean(self, option):\n",
    "        clean_option = ' '.join(t for t in option.split() \n",
    "                               if not self.isin(t))\n",
    "        return clean_option        \n",
    "    \n",
    "    def add(self, token):\n",
    "        self.junk_list.add(token)\n",
    "    \n",
    "    def remove(self, token):\n",
    "        self.junk_list.discard(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store queries results\n",
    "\n",
    "def save_data(data, filename, check=True):\n",
    "    if not filename.endswith('.pickle'):\n",
    "        filename += '.pickle'\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    if check:\n",
    "        with open(filename, 'rb') as handle:\n",
    "            loaded_data = pickle.load(handle)\n",
    "        assert data == loaded_data\n",
    "\n",
    "def load_data(filename):\n",
    "    if not filename.endswith('.pickle'):\n",
    "        filename += '.pickle'\n",
    "    with open(filename, 'rb') as handle:\n",
    "        return pickle.load(handle)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Results:\n",
    "    columns = [f'{q}_{m}' for q in ['test_query_1', 'test_query_2', 'test_query_3'] for m in ['recall', 'Rprec', 'ROC']]\n",
    "    table = pd.DataFrame(columns=columns)\n",
    "    table.index.name = 'method'\n",
    "\n",
    "#         if file_path:\n",
    "#             self.file_path = file_path\n",
    "#             self.prepare_file()\n",
    "    \n",
    "#     def prepare_file(self):\n",
    "#         if not os.path.exists(self.file_path):\n",
    "#             columns = ['method name']\n",
    "#             for q in self.queries:\n",
    "#                 for m in self.metrics:\n",
    "#                     columns.append(f'{q}_{m}')\n",
    "#             header = ','.join(columns)\n",
    "#             self.update_file(f'{header}\\n')\n",
    "    \n",
    "#     def update_file(self, info):\n",
    "#         if self.file_path:\n",
    "#             with open(self.file_path, 'a') as handle:\n",
    "#                 handle.write(info)\n",
    "    \n",
    "    def update(self, method, query, metric, score):\n",
    "        self.table.at[method, f'{query}_{metric}'] = score\n",
    "    \n",
    "    def show(self):\n",
    "        display(self.table)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('input.csv')\n",
    "\n",
    "class_counts = df.groupby('class').count().sort_index()\n",
    "N_true = class_counts['company name'].tolist()\n",
    "\n",
    "keys = list(map(str.lower, df['company name']))\n",
    "values = df['class']\n",
    "options = dict(zip(keys, values))\n",
    "\n",
    "n = len(options)\n",
    "\n",
    "queries = ['test_query_1', 'test_query_2', 'test_query_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_results(queries, distance_name='lcs', show_extra=2, threshold='cluster44', method_name='lcs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no junk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "junk_list = ['ltd', 'limited', 'ltda', 'llc', \n",
    "             'gbr', \n",
    "             'inc', 'corp', 'corporation', \n",
    "             'co', 'company',\n",
    "             '.']\n",
    "junk_cont = JunkContainer(junk_list)\n",
    "clean_options = {opt: junk_cont.clean(opt) for opt in options}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_results(queries, distance_name='lcs', option_identities=clean_options, show_extra=1, method_name='lcs (no junk)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_identities = Google().get_identities(queries)\n",
    "print(query_identities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# option_identities = Google().get_identities(options)\n",
    "# save_data(option_identities, 'option_identities')\n",
    "\n",
    "option_identities = load_data('option_identities')\n",
    "# pprint(option_identities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "found, not_found = [], []\n",
    "for opt in options:\n",
    "    identity = option_identities[opt]\n",
    "    (found if identity else not_found).append(opt)\n",
    "print(f'Found: {len(found)}')\n",
    "print(f'Not_found: {len(not_found)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_results(queries, \n",
    "              distance_name='fuzzy', \n",
    "              query_identities=query_identities, \n",
    "              option_identities=option_identities, \n",
    "              show_extra=5, \n",
    "              method_name='google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with {'types': 'Organization'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_params = {'types': 'Organization'}\n",
    "query_identities_TYPE = Google().get_identities(queries, extra_params)\n",
    "print(query_identities_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# option_identities_TYPE = Google().get_identities(options, extra_params)\n",
    "# save_data(option_identities_TYPE, 'option_identities_TYPE')\n",
    "\n",
    "option_identities_TYPE = load_data('option_identities_TYPE')\n",
    "# pprint(option_identities_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "found, not_found = [], []\n",
    "for opt in options:\n",
    "    identity = option_identities_TYPE[opt]\n",
    "    (found if identity else not_found).append(opt)\n",
    "print(f'Found: {len(found)}')\n",
    "print(f'Not_found: {len(not_found)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_results(queries, \n",
    "              distance_name='fuzzy', \n",
    "              query_identities=query_identities_TYPE, \n",
    "              option_identities=option_identities_TYPE, method_name='google (type=org)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# among suitable\n",
    "missing = [opt for opt, identity in option_identities_TYPE.items() \n",
    "           if not identity and options[opt]]\n",
    "pprint(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search by prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option_prefix_identities = Google().get_prefix_identities(options, extra_params={'types': 'Organization'})\n",
    "# save_data(option_prefix_identities, 'option_prefix_identities')\n",
    "\n",
    "option_prefix_identities = load_data('option_prefix_identities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "found, not_found = [], []\n",
    "for opt in options:\n",
    "    identity = option_prefix_identities[opt]\n",
    "    (found if any(identity) else not_found).append(opt)\n",
    "print(f'Found: {len(found)}')\n",
    "print(f'Not_found: {len(not_found)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_results(queries, \n",
    "              distance_name='prefix', \n",
    "              query_identities=query_identities_TYPE, \n",
    "              option_identities=option_prefix_identities,\n",
    "              show_extra=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search by prefixes, increase limit to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option_prefix_identities_x3 = Google().get_prefix_identities(options, extra_params={'limit':3, 'types': 'Organization'})\n",
    "# save_data(option_prefix_identities, 'option_prefix_identities')\n",
    "\n",
    "option_prefix_identities_x3 = load_data('option_prefix_identities_x3')\n",
    "# pprint(option_prefix_identities_x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_results(queries, \n",
    "              distance_name='prefix', \n",
    "              query_identities=query_identities_TYPE, \n",
    "              option_identities=option_prefix_identities_x3,\n",
    "              show_extra=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### search by prefixes, increase limit to 3, check also \"itemListElement.result.description.articleBody\" field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option_pref_id_x3_extended = Google().get_prefix_identities_extended(options, \n",
    "#                                                                      extra_params={'limit':3, 'types': 'Organization'})\n",
    "# save_data(option_pref_id_x3_extended, 'option_pref_id_x3_extended')\n",
    "\n",
    "option_pref_id_x3_extended = load_data('option_pref_id_x3_extended')\n",
    "pprint(option_pref_id_x3_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_results(queries, \n",
    "              distance_name='prefix', \n",
    "              query_identities=query_identities_TYPE, \n",
    "              option_identities=option_pref_id_x3_extended,\n",
    "              show_extra=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently not used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for opt in missing:\n",
    "    if options[opt] > 0:\n",
    "        print(opt)\n",
    "        print(wikipedia.search(opt))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content = wikipedia.page('Test_company').content.lower()\n",
    "print(content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = []\n",
    "tokens = ['token_1', 'token_2', 'token_3']\n",
    "for q in tokens:\n",
    "    i = -1\n",
    "    while True:\n",
    "        i = content.find(q, i+1)\n",
    "        if i < 0:\n",
    "            break\n",
    "        mentions.append((i, content[i-10:i+40]))\n",
    "pprint(mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### duckduckgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://api.duckduckgo.com/?q=simpsons+characters&format=json&pretty=1\n",
    "\n",
    "SERVICE_URL_DUCK = 'https://api.duckduckgo.com'\n",
    "PARAMS_DUCK = {\n",
    "    'format': 'json',\n",
    "    'pretty': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'query_1'\n",
    "params = dict(PARAMS_DUCK)\n",
    "params['q'] = query\n",
    "url = SERVICE_URL_DUCK + '?' + urllib.parse.urlencode(params)\n",
    "print(url)\n",
    "\n",
    "response = json.loads(urllib.request.urlopen(url).read())\n",
    "identity = response['Heading']\n",
    "print(identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(PARAMS_DUCK)\n",
    "for opt in missing:\n",
    "    query = opt\n",
    "    params['q'] = query\n",
    "    url = SERVICE_URL_DUCK + '?' + urllib.parse.urlencode(params)\n",
    "    response = json.loads(urllib.request.urlopen(url).read())\n",
    "    identity = response['Heading']\n",
    "    print(opt)\n",
    "    print(identity if identity else 'NOT FOUND')\n",
    "    print()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
